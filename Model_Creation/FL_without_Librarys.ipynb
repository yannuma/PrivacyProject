{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNsPZHNndLqcrLXaEQK353C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yannuma/PrivacyProject/blob/main/Model_Creation/FL_without_Librarys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irp2E1E94oPk",
        "outputId": "f9684475-5aea-4feb-8837-4ba8a8f138c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.4.1-py3-none-any.whl (226 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.3.0+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.11.4)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->opacus)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, opacus\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 opacus-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "import tarfile\n",
        "import torch\n",
        "import requests\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.utils.data as torch_data\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(f'https://www.comp.nus.edu.sg/~reza/files/dataset_texas.tgz')\n",
        "if response.status_code == 200:\n",
        "    with open(f'dataset_texas.tgz', 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(\"Download completed successfully.\")\n",
        "else:\n",
        "    print(f\"Failed to download file: {response.status_code}\")\n",
        "\n",
        "with tarfile.open(f'dataset_texas.tgz') as f:\n",
        "    f.extractall(f'data/')\n",
        "\n",
        "with open('data/texas/100/feats', 'r') as f:\n",
        "    features = f.readlines()\n",
        "with open('data/texas/100/labels', 'r') as f:\n",
        "    labels = f.readlines()\n",
        "\n",
        "print(len(features))\n",
        "print(len(labels))\n",
        "\n",
        "features_list = [list(map(int, ''.join(feature.split()).split(','))) for feature in features]\n",
        "labels_list = [int(label.strip()) for label in labels]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EhAcqrg5Gmw",
        "outputId": "92f09f3d-742b-46d2-810b-59d4235b08d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed successfully.\n",
            "67330\n",
            "67330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetSeq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetSeq, self).__init__()\n",
        "        self.fc1 = nn.Linear(6169, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 101)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def calc_accuracy(loader, network, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in loader:\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = network(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "    return (correct / total, total)"
      ],
      "metadata": {
        "id": "gngISfg18xXc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_client(trainloader, testloader, lr = 0.01, epochs = 2, network_output = True):\n",
        "    test_accs = []\n",
        "    max_accuracy = 0\n",
        "    best_model_path = 'best_model.pth'\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    network = NetSeq().to(device)\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr = lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    #print('Start training on', device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      #print('Epoch: {}'.format(epoch + 1))\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = network(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      # Validate all classes\n",
        "      with torch.no_grad():\n",
        "        acc_tuple = calc_accuracy(testloader, network, device)\n",
        "\n",
        "      #print('Accuracy of the network on %d test inputs: %d %%' % (acc_tuple[1], 100 * acc_tuple[0]))\n",
        "      test_accs.append(acc_tuple[0])\n",
        "      if acc_tuple[0] > max_accuracy:\n",
        "        max_accuracy = acc_tuple[0]\n",
        "        if network_output:\n",
        "          torch.save(network.state_dict(), best_model_path)\n",
        "\n",
        "    if network_output:\n",
        "      network.load_state_dict(torch.load(best_model_path))\n",
        "      network.to('cpu')\n",
        "      #print('Final accuracy: ', max_accuracy)\n",
        "      return (network, max_accuracy)\n",
        "    else:\n",
        "      #print('Final accuracy: ', max_accuracy)\n",
        "      return max_accuracy\n",
        "\n",
        "\n",
        "def train_client_DP(trainloader, testloader, lr = 0.001, epochs = 80, network_output = True, C = 3, epsilon = 10):\n",
        "\t\ttest_accs = []\n",
        "\t\tmax_accuracy = 0\n",
        "\t\tbest_model_path = 'best_model.pth'\n",
        "\t\tDelta = 1e-5\n",
        "\n",
        "\t\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\t\tnetwork = NetSeq().to(device)\n",
        "\t\tnetwork = ModuleValidator.fix(network)\n",
        "\t\tnetwork = network.to(device)\n",
        "\t\toptimizer = torch.optim.Adam(network.parameters(), lr = lr)\n",
        "\t\tcriterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tprivacy_engine = PrivacyEngine()\n",
        "\t\tnetwork, optimizer, trainloader = privacy_engine.make_private_with_epsilon(\n",
        "    \tmodule=network,\n",
        "    \toptimizer=optimizer,\n",
        "    \tdata_loader=trainloader,\n",
        "    \tmax_grad_norm=C,\n",
        "\t\t\ttarget_epsilon = epsilon,\n",
        "\t\t\ttarget_delta = Delta,\n",
        "\t\t\tepochs = epochs\n",
        "\t\t\t)\n",
        "\n",
        "\t\tnetwork.to(device)\n",
        "\n",
        "\t\t#print('Start training on', device)\n",
        "\t\tfor epoch in range(epochs):\n",
        "\n",
        "\t\t\t#print('Epoch: {}'.format(epoch + 1))\n",
        "\t\t\tfor i, data in enumerate(trainloader, 0):\n",
        "\t\t\t\tinputs, labels = data\n",
        "\t\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\toutputs = network(inputs)\n",
        "\t\t\t\tloss = criterion(outputs, labels)\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t# Validate all classes\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tacc_tuple = calc_accuracy(testloader, network, device)\n",
        "\n",
        "\t\t\t#print('Accuracy of the network on %d test inputs: %d %%' % (acc_tuple[1], 100 * acc_tuple[0]))\n",
        "\t\t\ttest_accs.append(acc_tuple[0])\n",
        "\t\t\tif acc_tuple[0] > max_accuracy:\n",
        "\t\t\t\tmax_accuracy = acc_tuple[0]\n",
        "\t\t\t\tif network_output:\n",
        "\t\t\t\t\ttorch.save(network.state_dict(), best_model_path)\n",
        "\n",
        "\t\tepsilon = privacy_engine.get_epsilon(delta=Delta)\n",
        "\t\t#print(\"Our Privacy Budget is:\", epsilon)\n",
        "\n",
        "\t\tif network_output:\n",
        "\t\t\tnetwork.load_state_dict(torch.load(best_model_path))\n",
        "\t\t\tnetwork.to('cpu')\n",
        "\t\t\t#print('Final accuracy: ', max_accuracy)\n",
        "\t\t\treturn (network, max_accuracy)\n",
        "\t\telse:\n",
        "\t\t\t#print('Final accuracy: ', max_accuracy)\n",
        "\t\t\treturn max_accuracy\n",
        "\n",
        "def aggregate_models(models):\n",
        "  for model in models:\n",
        "    model.eval()\n",
        "\n",
        "  num_models = len(models)\n",
        "  aggregated_params = {}\n",
        "\n",
        "  for model in models:\n",
        "      for name, param in model.named_parameters():\n",
        "          if name in aggregated_params:\n",
        "              aggregated_params[name] += param.data\n",
        "          else:\n",
        "              aggregated_params[name] = param.data.clone()\n",
        "\n",
        "  # Calculate the average of aggregated parameters\n",
        "  for name in aggregated_params:\n",
        "      aggregated_params[name] /= num_models\n",
        "\n",
        "  # Create a new model with the averaged parameters\n",
        "  aggregated_model = type(models[0])()\n",
        "  aggregated_model.load_state_dict(aggregated_params)\n",
        "\n",
        "  return aggregated_model"
      ],
      "metadata": {
        "id": "wn_WQwKW6_3h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients = 2\n",
        "\n",
        "dataset = TensorDataset(torch.tensor(features_list, dtype=torch.float), torch.tensor(labels_list, dtype=torch.long))\n",
        "client_size = len(dataset) // clients\n",
        "sizes = [client_size] * (clients - 1) + [len(dataset) - client_size * (clients - 1)]\n",
        "client_datasets = random_split(dataset, sizes)\n",
        "train_loaders = []\n",
        "test_loaders = []\n",
        "batch = 64\n",
        "\n",
        "for client_dataset in client_datasets:\n",
        "    train_size = int(0.8 * len(client_dataset))\n",
        "    train_subset, test_subset = torch.utils.data.random_split(client_dataset, [train_size, len(client_dataset) - train_size])\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
        "    train_loaders.append(train_loader)\n",
        "    test_loader = DataLoader(test_subset, batch_size=batch, shuffle=False)\n",
        "    test_loaders.append(test_loader)"
      ],
      "metadata": {
        "id": "EgZ1BUsh772z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = [5, 10, 20, 40, 60]\n",
        "\n",
        "results = []\n",
        "agg_model = []\n",
        "for epoch in epochs:\n",
        "  client_models = []\n",
        "\n",
        "  #Train the clients\n",
        "  for client in range(clients):\n",
        "    res = train_client(train_loaders[client], test_loaders[client], 0.001, epoch)\n",
        "    client_models.append(res[0])\n",
        "    results.append({'Mode': \"Training Client\", 'client': client, 'epoch': epoch, 'accuracy': res[1]})\n",
        "\n",
        "  #aggegrate the model\n",
        "  aggegrated_model = aggregate_models(client_models)\n",
        "  agg_model.append(aggegrated_model)\n",
        "  #calculate the accuracy for each client testset\n",
        "  for client in range(clients):\n",
        "      with torch.no_grad():\n",
        "        acc = calc_accuracy(test_loaders[client], aggegrated_model, device = 'cpu')\n",
        "        results.append({'Mode': \"Aggregated Model\", 'client': client, 'epoch': epoch, 'accuracy': acc[0]})\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "agg_model_df = df[df['Mode'] == \"Aggregated Model\"]\n",
        "average_accuracy_per_epoch = agg_model_df.groupby('epoch')['accuracy'].mean()\n",
        "max_epoch = average_accuracy_per_epoch.idxmax()\n",
        "max_average_accuracy = average_accuracy_per_epoch.max()\n",
        "\n",
        "print(\"Average Accuracy per Epoch for Aggregated Model:\")\n",
        "print(average_accuracy_per_epoch)\n",
        "print(\"\\nMaximum Average Accuracy for Aggregated Model:\")\n",
        "print(f\"Epoch: {max_epoch}, Accuracy: {max_average_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3CfyBfY_WGx",
        "outputId": "76b93532-dcb9-44be-df67-a57e7398399f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy per Epoch for Aggregated Model:\n",
            "epoch\n",
            "5     0.585177\n",
            "10    0.574113\n",
            "20    0.571513\n",
            "40    0.602555\n",
            "60    0.595723\n",
            "Name: accuracy, dtype: float64\n",
            "\n",
            "Maximum Average Accuracy for Aggregated Model:\n",
            "Epoch: 40, Accuracy: 0.6025545819099956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_index = epochs.index(max_epoch)\n",
        "best_model = agg_model[epoch_index]\n",
        "torch.save(best_model.state_dict(), 'model_FL.pth')\n",
        "from google.colab import files\n",
        "files.download('model_FL.pth')"
      ],
      "metadata": {
        "id": "CWgQOWadLRTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5dc1e125-06bb-433a-d61b-fb556bb0d7a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e8a72401-62ff-4e9e-aa8e-c6bd07a8e457\", \"model_FL.pth\", 3279762)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Todo everything from here\n",
        "\n",
        "epsilons = [1, 10, 20]\n",
        "epochs = [5, 10, 20, 40, 60]\n",
        "results_DP = []\n",
        "for eps in epsilons:\n",
        "  for epoch in epochs:\n",
        "    client_models = []\n",
        "    for client in range(clients):\n",
        "      res = (train_client_DP(train_loaders[client], test_loaders[client], epochs = epoch, epsilon = eps))\n",
        "      client_models.append(res[0]._module)\n",
        "      results_DP.append({'Mode': \"Training Client\", 'client': client, 'epsilon': eps,'epoch': epoch, 'accuracy': res[1]})\n",
        "\n",
        "    aggegrated_model = aggregate_models(client_models)\n",
        "\n",
        "    for client in range(clients):\n",
        "        with torch.no_grad():\n",
        "          #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "          device = 'cpu'\n",
        "          acc = calc_accuracy(test_loaders[client], aggegrated_model, device)\n",
        "          results_DP.append({'Mode': \"Aggregated Model\", 'client': client, 'epsilon': eps,'epoch': epoch, 'accuracy': acc[0]})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nguZ0LrN1cmH",
        "outputId": "06df322c-e5fb-4fb0-eacf-f855671ae2dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_DP = pd.DataFrame(results_DP)\n",
        "agg_model_df = df_DP[df_DP['Mode'] == \"Aggregated Model\"]\n",
        "average_accuracy_per_epoch_epsilon = agg_model_df.groupby(['epoch', 'epsilon'])['accuracy'].mean()\n",
        "print(average_accuracy_per_epoch_epsilon)\n",
        "\n",
        "max_accuracy_per_epsilon = average_accuracy_per_epoch_epsilon.groupby('epsilon').idxmax().apply(\n",
        "    lambda x: (x[0], average_accuracy_per_epoch_epsilon.loc[x])\n",
        ")\n",
        "max_accuracy_per_epsilon = max_accuracy_per_epsilon.rename(\"epoch, accuracy\").reset_index()\n",
        "print(\"Maximum Average Accuracy per Epsilon:\")\n",
        "print(max_accuracy_per_epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39R3nqialHy_",
        "outputId": "955db534-083e-4154-ccdf-d69620dde066"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  epsilon\n",
            "5      1          0.157285\n",
            "       10         0.246101\n",
            "       20         0.286722\n",
            "10     1          0.243873\n",
            "       10         0.315535\n",
            "       20         0.337591\n",
            "20     1          0.293406\n",
            "       10         0.401753\n",
            "       20         0.425590\n",
            "40     1          0.311451\n",
            "       10         0.414228\n",
            "       20         0.435764\n",
            "60     1          0.295411\n",
            "       10         0.388460\n",
            "       20         0.424254\n",
            "Name: accuracy, dtype: float64\n",
            "Maximum Average Accuracy per Epsilon:\n",
            "   epsilon            epoch, accuracy\n",
            "0        1   (40, 0.3114510619337591)\n",
            "1       10   (40, 0.4142284271498589)\n",
            "2       20  (40, 0.43576414673993763)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epsilons = [1, 10, 20]\n",
        "epoch = 40\n",
        "for eps in epsilons:\n",
        "  client_models = []\n",
        "  for client in range(clients):\n",
        "    res = (train_client_DP(train_loaders[client], test_loaders[client], epochs = epoch, epsilon = eps))\n",
        "    client_models.append(res[0]._module)\n",
        "\n",
        "  aggegrated_model = aggregate_models(client_models)\n",
        "  torch.save(aggegrated_model.state_dict(), f'model_FL_epsilon_{eps}.pth')\n",
        "  files.download(f'model_FL_epsilon_{eps}.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "clHmyxgbnuhV",
        "outputId": "3ec98a57-7ad4-4f34-9116-d43eea6eba65"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_37e25993-e1ac-498b-afec-b5f3197d3e81\", \"model_FL_epsilon_1.pth\", 3279926)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ce7de037-8558-445f-98a6-31f40c0268e7\", \"model_FL_epsilon_10.pth\", 3279936)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_410284b2-0bab-4e2b-8f31-5ee3acc3a5c4\", \"model_FL_epsilon_20.pth\", 3279936)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}