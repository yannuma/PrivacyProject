{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0f099a-ab71-4570-a7f3-2b4eea0a989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a458269-56fe-42ba-9cb3-7f94584abbd6",
   "metadata": {},
   "source": [
    "##  Adversarial Attacks: Fast Gradient Sign Method\n",
    "\n",
    "The Fast Gradient Sign Method (FGSM) is a simple yet effective method to generate adversarial examples, which are inputs to a machine learning model that are intentionally designed to cause the model to make a mistake. FGSM is often used to test the robustness of machine learning models against such adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6130b7f-a0e2-4a4d-b4b7-0c8904793c8e",
   "metadata": {},
   "source": [
    "How FGSM Works\n",
    "\n",
    "The FGSM method perturbs the original input data in the direction that maximizes the loss of the model. This is done by taking a single step in the direction of the gradient of the loss with respect to the input.\n",
    "\n",
    "Steps of FGSM\n",
    "1. Compute the Loss: Given an input data point \n",
    "2. Compute the Gradient: Compute the gradient of the loss with respect to the input data\n",
    "3. Create the Perturbation: Create the perturbation by taking the sign of the gradient and multiplying it by a small constant \n",
    "4. Generate the Adversarial Example: Add the perturbation to the original input to create the adversarial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8cde7f9-1892-40ef-a280-f528d63c6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and labels\n",
    "with open('data/texas/100/feats', 'r') as f:\n",
    "    features = f.readlines()\n",
    "with open('data/texas/100/labels', 'r') as f:\n",
    "    labels = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "839b15cf-7630-4637-81b1-cf7d814c401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example features and labels list\n",
    "features_list = [list(map(int, ''.join(feature.split()).split(','))) for feature in features]\n",
    "labels_list = [int(label.strip()) for label in labels]\n",
    "\n",
    "# Convert lists to tensors\n",
    "all_features_tensor = torch.tensor(features_list, dtype=torch.float)\n",
    "all_labels_tensor = torch.tensor(labels_list, dtype=torch.long)\n",
    "\n",
    "# Define the number of points for train, test, and population\n",
    "num_total_points = len(all_features_tensor)\n",
    "num_train_points = int(0.8 * num_total_points)\n",
    "num_test_points = int(0.1 * num_total_points)\n",
    "num_population_points = num_total_points - num_train_points - num_test_points\n",
    "\n",
    "# Create explicit indices for the splits\n",
    "train_indices = np.arange(0, num_train_points)\n",
    "test_indices = np.arange(num_train_points, num_train_points + num_test_points)\n",
    "population_indices = np.arange(num_train_points + num_test_points, num_total_points)\n",
    "\n",
    "# Create TensorDatasets based on the indices\n",
    "train_data = TensorDataset(all_features_tensor[train_indices], all_labels_tensor[train_indices])\n",
    "test_data = TensorDataset(all_features_tensor[test_indices], all_labels_tensor[test_indices])\n",
    "population_data = TensorDataset(all_features_tensor[population_indices], all_labels_tensor[population_indices])\n",
    "\n",
    "# Load model\n",
    "class NetSeq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetSeq, self).__init__()\n",
    "        self.fc1 = nn.Linear(6169, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 101)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model_S = NetSeq()\n",
    "model_Y = NetSeq()\n",
    "model_E = NetSeq()\n",
    "model_X = NetSeq()\n",
    "\n",
    "# Load the trained parameters\n",
    "model_S.load_state_dict(torch.load('../Models/model_S.pth'))\n",
    "model_Y.load_state_dict(torch.load('../Models/model_Y_saved_properly.pth'))\n",
    "model_E.load_state_dict(torch.load('../Models/model_E_saved_properly.pth'))\n",
    "model_X.load_state_dict(torch.load('../Models/model_X_saved_properly.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_S.eval()\n",
    "model_Y.eval()\n",
    "model_E.eval()\n",
    "model_X.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21dac50c-bab8-4641-99e1-3ba0175f4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 63.52%\n",
      "Model S: Epsilon: 0\tAccuracy: 63.52%\n",
      "Epsilon: 0.01\tTest Accuracy = 1.60%\n",
      "Model S: Epsilon: 0.01\tAccuracy: 1.60%\n",
      "Epsilon: 0.05\tTest Accuracy = 0.00%\n",
      "Model S: Epsilon: 0.05\tAccuracy: 0.00%\n",
      "Epsilon: 0\tTest Accuracy = 45.03%\n",
      "Model Y: Epsilon: 0\tAccuracy: 45.03%\n",
      "Epsilon: 0.01\tTest Accuracy = 0.10%\n",
      "Model Y: Epsilon: 0.01\tAccuracy: 0.10%\n",
      "Epsilon: 0.05\tTest Accuracy = 0.00%\n",
      "Model Y: Epsilon: 0.05\tAccuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# FGSM attack function\n",
    "def fgsm_attack(data, epsilon, data_grad):\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_data = data + epsilon * sign_data_grad\n",
    "    perturbed_data = torch.clamp(perturbed_data, 0, 1)\n",
    "    return perturbed_data\n",
    "\n",
    "\n",
    "def test_fgsm(model, device, test_loader, epsilon):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data.requires_grad = True\n",
    "\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1].squeeze()\n",
    "\n",
    "        # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "        mask = init_pred.eq(target)\n",
    "        if mask.sum().item() == 0:\n",
    "            continue\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "        output = model(perturbed_data)\n",
    "        final_pred = output.max(1, keepdim=True)[1].squeeze()\n",
    "\n",
    "        correct += final_pred.eq(target).sum().item()\n",
    "\n",
    "        if len(adv_examples) < 5:\n",
    "            # Save some examples for visualization later\n",
    "            adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "            for i in range(min(len(adv_ex), 5 - len(adv_examples))):\n",
    "                adv_examples.append((init_pred[i].item(), final_pred[i].item(), adv_ex[i]))\n",
    "\n",
    "    final_acc = correct / float(len(test_loader.dataset))\n",
    "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {final_acc * 100:.2f}%\")\n",
    "    return final_acc, adv_examples\n",
    "\n",
    "\n",
    "# Define epsilon values for testing\n",
    "epsilons = [0, 0.01, 0.05]\n",
    "\n",
    "# Load the model onto the device\n",
    "model_S.to(device)\n",
    "\n",
    "# Test the model with FGSM attack\n",
    "for eps in epsilons:\n",
    "    acc, ex = test_fgsm(model_S, device, test_loader, eps)\n",
    "    print(f\"Model S: Epsilon: {eps}\\tAccuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the model onto the device\n",
    "model_Y.to(device)\n",
    "\n",
    "# Test the model with FGSM attack\n",
    "for eps in epsilons:\n",
    "    acc, ex = test_fgsm(model_Y, device, test_loader, eps)\n",
    "    print(f\"Model Y: Epsilon: {eps}\\tAccuracy: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664ec91-2c30-42f1-a1ef-7f6159ce5ec0",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The results indicate that the models performs reasonably well on the unperturbed test data (epsilon = 0), achieving an accuracy of 63.52%\\45%. However, when adversarial perturbations are introduced, even with a small epsilon value of 0.05, the model's accuracy drops to 0.00%.\n",
    "\n",
    "This suggests that the models is highly vulnerable to adversarial attacks. Even small perturbations are enough to cause it to misclassify all the test samples.\n",
    "\n",
    "\n",
    "Possible Reasons and Actions\n",
    "1. High Sensitivity to Perturbations:\n",
    "\n",
    "    The model might be overfitted to the training data and lacks robustness to slight changes in the input.\n",
    "   \n",
    "    Action: Consider using data augmentation, adversarial training, or regularization techniques to improve robustness.\n",
    "\n",
    "3. Magnitude of Perturbations:\n",
    "\n",
    "    The perturbations introduced by the FGSM attack might be too large relative to the input scale.\n",
    "   \n",
    "    Action: Verify the input data normalization and ensure the perturbations are appropriately scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eccd04e-3eda-4b5e-8701-2840067b1ff5",
   "metadata": {},
   "source": [
    "## Adversarial Attacks: Projected Gradient Descent (PGD)\n",
    "\n",
    "Projected Gradient Descent (PGD) is an iterative adversarial attack method that refines the Fast Gradient Sign Method (FGSM) approach by iteratively applying small perturbations to the input data. Steps:\n",
    "\n",
    "Initialization: Start with the original input.\n",
    "\n",
    "Iteration:\n",
    "1. Calculate the gradient of the loss with respect to the input data.\n",
    "2. Apply a small perturbation in the direction of the gradient (using the sign of the gradient for direction).\n",
    "3. Project the perturbed data back into the valid data range (e.g., for images, pixel values should be between 0 and 1) and within an epsilon-ball around the original input.\n",
    "4. Repetition: Repeat the above steps for a specified number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de36a565-65d0-4083-b456-a7be6c786850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, data, target, epsilon, alpha, num_iter):\n",
    "    # Clone the data tensor\n",
    "    perturbed_data = data.clone().detach().requires_grad_(True).to(device)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        output = model(perturbed_data)\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = perturbed_data.grad.data\n",
    "        \n",
    "        # Apply gradient ascent step\n",
    "        perturbed_data = perturbed_data + alpha * data_grad.sign()\n",
    "        \n",
    "        # Clip perturbations to be within the epsilon ball\n",
    "        perturbed_data = torch.max(torch.min(perturbed_data, data + epsilon), data - epsilon)\n",
    "        \n",
    "        # Ensure data is within valid range\n",
    "        perturbed_data = torch.clamp(perturbed_data, 0, 1)\n",
    "        \n",
    "        # Clear the gradients for the next iteration\n",
    "        perturbed_data = Variable(perturbed_data.data, requires_grad=True)\n",
    "    \n",
    "    return perturbed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d3a7b3c-3499-46f1-8119-69b3c755589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_S PGD Attack\n",
      "Epsilon: 0.01\tAlpha: 0.01\tIterations: 40\tTest Accuracy = 0.25%\n",
      "Model_Y PGD Attack\n",
      "Epsilon: 0.01\tAlpha: 0.01\tIterations: 40\tTest Accuracy = 0.00%\n",
      "Model_E PGD Attack\n",
      "Epsilon: 0.01\tAlpha: 0.01\tIterations: 40\tTest Accuracy = 0.04%\n",
      "Model_X PGD Attack\n",
      "Epsilon: 0.01\tAlpha: 0.01\tIterations: 40\tTest Accuracy = 0.07%\n"
     ]
    }
   ],
   "source": [
    "def pgd_attack(model, data, target, epsilon, alpha, num_iter):\n",
    "    # Clone the data tensor\n",
    "    perturbed_data = data.clone().detach().requires_grad_(True).to(device)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        output = model(perturbed_data)\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = perturbed_data.grad.data\n",
    "        \n",
    "        # Apply gradient ascent step\n",
    "        perturbed_data = perturbed_data + alpha * data_grad.sign()\n",
    "        \n",
    "        # Clip perturbations to be within the epsilon ball\n",
    "        perturbed_data = torch.max(torch.min(perturbed_data, data + epsilon), data - epsilon)\n",
    "        \n",
    "        # Ensure data is within valid range\n",
    "        perturbed_data = torch.clamp(perturbed_data, 0, 1)\n",
    "        \n",
    "        # Clear the gradients for the next iteration\n",
    "        perturbed_data = Variable(perturbed_data.data, requires_grad=True)\n",
    "    \n",
    "    return perturbed_data\n",
    "\n",
    "def test_pgd(model, device, test_loader, epsilon, alpha, num_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Generate adversarial examples using PGD\n",
    "        perturbed_data = pgd_attack(model, data, target, epsilon, alpha, num_iter)\n",
    "        \n",
    "        # Re-classify the perturbed images\n",
    "        output = model(perturbed_data)\n",
    "        final_pred = output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # Check for correct classification\n",
    "        correct += final_pred.eq(target.view_as(final_pred)).sum().item()\n",
    "        \n",
    "        # Save some examples for visualization\n",
    "        for i in range(len(data)):\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data[i].squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append((final_pred[i].item(), target[i].item(), adv_ex))\n",
    "    \n",
    "    final_acc = correct / float(len(test_loader.dataset))\n",
    "    print(f\"Epsilon: {epsilon}\\tAlpha: {alpha}\\tIterations: {num_iter}\\tTest Accuracy = {final_acc * 100:.2f}%\")\n",
    "    return final_acc, adv_examples\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "batch_size = 32\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the model onto the device\n",
    "model_S.to(device)\n",
    "model_Y.to(device)\n",
    "model_E.to(device)\n",
    "model_X.to(device)\n",
    "\n",
    "# Define parameters for the PGD attack\n",
    "epsilon = 0.01\n",
    "alpha = 0.01\n",
    "num_iter = 40\n",
    "\n",
    "# Test each model with PGD attack\n",
    "print(\"Model_S PGD Attack\")\n",
    "acc, ex = test_pgd(model_S, device, test_loader, epsilon, alpha, num_iter)\n",
    "\n",
    "print(\"Model_Y PGD Attack\")\n",
    "acc, ex = test_pgd(model_Y, device, test_loader, epsilon, alpha, num_iter)\n",
    "\n",
    "print(\"Model_E PGD Attack\")\n",
    "acc, ex = test_pgd(model_E, device, test_loader, epsilon, alpha, num_iter)\n",
    "\n",
    "print(\"Model_X PGD Attack\")\n",
    "acc, ex = test_pgd(model_X, device, test_loader, epsilon, alpha, num_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ee7be-d96b-42d8-83e9-7396be99de0b",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "The test accuracy for all models after applying the PGD attack is 0.00%. This means that the PGD attack is very effective in generating adversarial examples that fool the models completely. Several reasons can explain this outcome:\n",
    "\n",
    "1. Effective Attack: PGD is a strong adversarial attack that iteratively applies small perturbations to the input, making it more likely to find a perturbation that causes misclassification compared to simpler attacks like FGSM.\n",
    "2. Model Vulnerability: The models may not have been trained with adversarial robustness in mind, making them highly susceptible to adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8fa7d-bec9-4cd7-9e8c-9a492fac44c7",
   "metadata": {},
   "source": [
    "#### Compare:\n",
    "\n",
    "Fast Gradient Sign Method:\n",
    "\n",
    "FGSM is a single-step attack.\n",
    "It perturbs the input data by a fixed amount in the direction of the gradient of the loss with respect to the input.\n",
    "Formula: $[ x_{\\text{adv}} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y)) ]$\n",
    "\n",
    "Here, $(x_{\\text{adv}})$ is the adversarial example, $(\\epsilon)$ is the perturbation size, (x) is the original input, $(J(\\theta, x, y))$ is the loss function, and $(\\nabla_x)$ represents the gradient with respect to (x).\n",
    "\n",
    "\n",
    "PGD (Projected Gradient Descent:\n",
    "PGD improves upon FGSM by applying perturbations iteratively.\n",
    "It initializes the example to a random point within a specified ball (determined by the L∞ norm) and performs multiple iterations.\n",
    "Formula for PGD update at iteration (t): $[ x_{\\text{adv}}^{(t+1)} = \\text{Proj}\\left(x + \\epsilon \\left(x_{\\text{adv}}^{(t)} + \\alpha \\cdot \\text{sign}(\\nabla_x J(\\theta, x_{\\text{adv}}^{(t)}, y))\\right)\\right) ]$\n",
    "Here, (\\alpha) is the step size, and (\\text{Proj}) denotes projection onto the (\\epsilon)-ball around the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9416d3-43fe-4787-9db7-a06f9f227977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
